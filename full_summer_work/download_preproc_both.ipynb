{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec14512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 22:39:55.881302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754451595.894257  135193 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754451595.897761  135193 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754451595.907716  135193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754451595.907741  135193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754451595.907743  135193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754451595.907744  135193 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Download and load QUASR device database\n",
    "import gzip\n",
    "import json\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor\n",
    "import time\n",
    "import numpy as np\n",
    "from simsopt.field import Current\n",
    "from simsopt.geo import SurfaceRZFourier\n",
    "from simsopt._core import load\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a499e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMSOPT_DIR = 'mini_quasr_simsopt_files'\n",
    "LOG_CSV = \"mini_quasr_log.csv\"\n",
    "data_dir = Path('quasr_simsopt_files')\n",
    "output_dir = Path('mini_surface_coil_tfrecords')\n",
    "os.makedirs(SIMSOPT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_COILS = 6\n",
    "FEATURES_PER_COIL = 100\n",
    "NUM_THREAD_WORKERS = 8\n",
    "NUM_PROCESS_WORKERS = 1\n",
    "CHUNK_SIZE = 3000\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 2  # seconds\n",
    "MAX_NFP = 5\n",
    "MAX_COEFS = 441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f92a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://quasr.flatironinstitute.org/database.json.gz\"\n",
    "# print('Downloading device database...')\n",
    "# r = requests.get(url)\n",
    "# r.raise_for_status()\n",
    "\n",
    "# with gzip.open(BytesIO(r.content), 'rt', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# df = pd.DataFrame(**data)\n",
    "# print(f\"Loaded {len(df)} devices.\")\n",
    "\n",
    "# df.to_hdf('QUASR_Stellarators.h5', key = 'full_dataset')\n",
    "df = pd.read_hdf('QUASR_Stellarators.h5', key = 'full_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b644101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314309 devices match your criteria.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Apply filters to select matching devices\n",
    "filtered = df[\n",
    "    (df[\"Nfourier_coil\"] == 16) &\n",
    "    (df['qs_error'] >= -4) &\n",
    "    # (df[\"max_elongation\"] <= 10) &\n",
    "    # (df[\"aspect_ratio\"] >= 4) & (df[\"aspect_ratio\"] <= 10) &\n",
    "    (df[\"nc_per_hp\"] >= 1) & (df[\"nc_per_hp\"] <= 6) &\n",
    "    (df[\"nfp\"] >= 1) & (df[\"nfp\"] <= 5)\n",
    "].copy()\n",
    "\n",
    "print(f\"{len(filtered)} devices match your criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c0d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 3000\n",
    "indices = np.linspace(0, len(filtered)-1, n_points, dtype=int)\n",
    "filtered = filtered.iloc[indices]\n",
    "# filtered = pd.read_hdf('QUASR_Stellarators.h5', key = 'general_filter_sample_3000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6899ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simsopt_url(device_id):\n",
    "    pid = device_id.zfill(7)\n",
    "    return f\"https://quasr.flatironinstitute.org/simsopt_serials/{pid[:4]}/serial{pid}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "067ac04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Robust download with retries\n",
    "def download_with_retries(url: str, path: str) -> bool:\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=30)\n",
    "            if r.status_code == 200:\n",
    "                with open(path, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"{url} returned status {r.status_code} (attempt {attempt})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on {url} (attempt {attempt}): {e}\")\n",
    "        time.sleep(RETRY_DELAY)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c880e329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 devices to download in 1 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Prepare log and list of device IDs to download\n",
    "if os.path.exists(LOG_CSV):\n",
    "    log_df = pd.read_csv(LOG_CSV, dtype=str)\n",
    "else:\n",
    "    log_df = pd.DataFrame(columns=[\"ID\", 'simsopt_url', \"status\"])\n",
    "\n",
    "processed = set(log_df[\"ID\"])\n",
    "device_ids = [str(d) for d in filtered[\"ID\"] if str(d) not in processed] #this is where you change which df you want the device ids from\n",
    "chunks = [device_ids[i:i+CHUNK_SIZE] for i in range(0, len(device_ids), CHUNK_SIZE)]\n",
    "print(f\"{len(device_ids)} devices to download in {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7969a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_device(dev_id):\n",
    "        pid = dev_id.zfill(7)\n",
    "        # vmec_path = os.path.join(VMEC_DIR, f\"input.{pid}\")\n",
    "        simsopt_path = os.path.join(SIMSOPT_DIR, f\"input_{pid}.json\")\n",
    "        # vmec_ok = os.path.exists(vmec_path) or download_with_retries(vmec_url(dev_id), vmec_path)\n",
    "        simsopt_ok = os.path.exists(simsopt_path) or download_with_retries(simsopt_url(dev_id), simsopt_path)\n",
    "        status = \"success\" if simsopt_ok else 'failed'\n",
    "        return {\n",
    "            \"ID\": dev_id,\n",
    "            # \"vmec_url\": vmec_url(dev_id),\n",
    "            'simsopt_url': simsopt_url(dev_id),\n",
    "            \"status\": status\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea6824a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value.flatten()))\n",
    "\n",
    "def _int_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def serialize_data(id: str, coils: np.ndarray, coil_mask: np.ndarray, surface: np.ndarray, surface_mask: np.ndarray):\n",
    "    feature = {\n",
    "        'ID': _bytes_feature(id.encode('utf-8')),\n",
    "        'coil_data': _float_feature(coils),\n",
    "        'coil_mask': _int_feature(coil_mask),\n",
    "        'surface_data': _float_feature(surface),\n",
    "        'surface_mask': _int_feature(surface_mask)\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f853cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_coils(file_path):\n",
    "    try:\n",
    "        id = str(file_path)[-12:-5]\n",
    "        surfaces, coils = load(str(file_path))\n",
    "        s = surfaces[-1]\n",
    "\n",
    "        num_coils = len(coils) // (s.nfp * 2)\n",
    "        num_coils = min(num_coils, MAX_COILS)\n",
    "\n",
    "        log_scaler = np.log10(coils[0].current.scale)\n",
    "        scaler_token = np.full((1, FEATURES_PER_COIL), log_scaler, dtype=np.float32)\n",
    "\n",
    "        coil_array = np.zeros((MAX_COILS + 1, FEATURES_PER_COIL), dtype=np.float32)\n",
    "        for i in range(num_coils):\n",
    "            params = coils[i].x[-99:]  # 99 Fourier + 1 current\n",
    "            curr = np.array(coils[i].current.current_to_scale.current)\n",
    "            coil_array[i] = np.append(curr, params)\n",
    "\n",
    "        coil_array[-1] = scaler_token  # context token\n",
    "\n",
    "        coil_mask = np.array([1] * num_coils + [0] * (MAX_COILS - num_coils), dtype=np.int64)\n",
    "\n",
    "        return id, coil_array, coil_mask #serialize_coil(id, coil_array, coil_mask)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {file_path}: {e}\")\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0320b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_surface(file_path):\n",
    "    \"\"\"\n",
    "    Convert s.x and metadata to [N_modes, 5] array:\n",
    "    columns = [m_norm, n_norm, is_cos, is_R, coeff_value]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        id = str(file_path)[-12:-5]\n",
    "        surfaces, coils = load(str(file_path))\n",
    "        outer_surface = surfaces[-1]\n",
    "        s = outer_surface.to_RZFourier() #the bottleneck in speed\n",
    "\n",
    "        x = s.x  # the coeff vector\n",
    "        nfp = s.nfp\n",
    "\n",
    "        num_coefs = len(x)\n",
    "        if num_coefs > MAX_COEFS:\n",
    "            print(num_coefs)\n",
    "        \n",
    "        m = s.m  # mode numbers, shape (N_modes,)\n",
    "        n = s.n\n",
    "        \n",
    "        num_modes = (len(m)+1)//2\n",
    "        # Normalize mode indices\n",
    "        max_m = np.max(np.abs(m)) or 1\n",
    "        max_n = np.max(np.abs(n)) or 1\n",
    "\n",
    "        # Type flags\n",
    "        is_cos = np.concatenate([\n",
    "            np.ones(num_modes, dtype=bool),  # R_cos\n",
    "            np.zeros(num_modes-1, dtype=bool)  # Z_sin\n",
    "        ])\n",
    "\n",
    "        surface_set = np.zeros((MAX_COEFS+1, 4), dtype=np.float32)\n",
    "        surface_mask = np.zeros((MAX_COEFS,), dtype=np.int64)\n",
    "        nfp_norm = float(nfp) / MAX_NFP \n",
    "\n",
    "        for i in range(min(num_coefs, MAX_COEFS)):\n",
    "            surface_set[i] = [\n",
    "                m[i] / max_m,\n",
    "                n[i] / max_n,\n",
    "                float(is_cos[i]),\n",
    "                x[i]\n",
    "            ]\n",
    "            surface_mask[i] = 1\n",
    "\n",
    "        surface_set[-1] = nfp_norm\n",
    "            \n",
    "        return surface_set, surface_mask #serialize_surface(id, surface_set, surface_mask)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1442a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    id, coil_array, coil_mask = process_coils(file_path)\n",
    "    surface_set, surface_mask = process_surface(file_path)\n",
    "    return serialize_data(id, coil_array, coil_mask, surface_set, surface_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf6137cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord_chunk(serialized_examples, output_path):\n",
    "    with tf.io.TFRecordWriter(str(output_path)) as writer:\n",
    "        for ex in serialized_examples:\n",
    "            if ex:\n",
    "                writer.write(ex)\n",
    "\n",
    "def datasets_to_tfrecords(directory: Path, output_dir: Path, idx,\n",
    "                               chunk_size=CHUNK_SIZE, num_workers=NUM_PROCESS_WORKERS):\n",
    "    files = list(directory.glob(\"*.json\"))\n",
    "    total_files = len(files)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        serialized_examples = list(tqdm(\n",
    "            executor.map(process_file, files),\n",
    "            total=total_files,\n",
    "            desc=f\"Chunk {idx//chunk_size:03d}\"\n",
    "        ))\n",
    "\n",
    "    serialized_examples = [ex for ex in serialized_examples if ex is not None]\n",
    "\n",
    "    output_path = output_dir / f\"surface_coil_chunk_{idx:03d}.tfrecord\"\n",
    "\n",
    "    write_tfrecord_chunk(serialized_examples, output_path)\n",
    "    print(f\"✅ Saved {len(serialized_examples)} surface coil pair samples to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f18eadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Chunk 1/1: 3000 devices ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 1: 100%|██████████| 3000/3000 [05:08<00:00,  9.72it/s]\n",
      "Chunk 000: 100%|██████████| 3000/3000 [16:04<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 3000 surface coil pair samples to mini_surface_coil_tfrecords\n",
      "Chunk 1 completed: 3000/3000 successful.\n"
     ]
    }
   ],
   "source": [
    "for idx, chunk in enumerate(chunks, start=1):\n",
    "    print(f\"\\n=== Chunk {idx}/{len(chunks)}: {len(chunk)} devices ===\")\n",
    "    results = []\n",
    "\n",
    "    os.makedirs(SIMSOPT_DIR, exist_ok=True)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=NUM_THREAD_WORKERS) as executor:\n",
    "        futures = {executor.submit(process_device, dev): dev for dev in chunk}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=f\"Chunk {idx}\"):\n",
    "            results.append(fut.result())\n",
    "\n",
    "    datasets_to_tfrecords(directory=data_dir, output_dir=output_dir, idx=idx)\n",
    "\n",
    "    log_df = pd.concat([log_df, pd.DataFrame(results)], ignore_index=True)\n",
    "    log_df.to_csv(LOG_CSV, index=False)\n",
    "    success = sum(r[\"status\"] == \"success\" for r in results)\n",
    "    print(f\"Chunk {idx} completed: {success}/{len(results)} successful.\")\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(SIMSOPT_DIR)\n",
    "    except OSError as e:\n",
    "        print(f'Error deleting directory: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFCoil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
