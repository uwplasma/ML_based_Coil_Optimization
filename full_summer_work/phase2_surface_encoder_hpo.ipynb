{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd233e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Suppress TensorFlow logs\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Suppress all but fatal errors\n",
    "\n",
    "# Optional: Disable oneDNN info message\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "# Optional: Disable XLA to reduce cu* factory warnings\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceaaceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 23:20:05.973777: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754454005.990144  143922 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754454005.993994  143922 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754454006.004100  143922 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754454006.004126  143922 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754454006.004128  143922 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754454006.004129  143922 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "E0000 00:00:1754454008.229626  143922 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1754454008.230210  143922 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/isaac/anaconda3/envs/TFCoil/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-05 23:20:08,594\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "tf.constant(1.0)  # Trigger basic op\n",
    "import logging\n",
    "logging.getLogger('absl').setLevel(logging.ERROR)\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3ca2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from latent_loader import load_full_dataset, load_split_datasets\n",
    "from transformers import TransformerEncoder, TransformerDecoder\n",
    "from SurfaceEncoder import SurfaceEncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f0b755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecord_dir = Path(\"mini_latents_tfrecords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32489572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(max_sets, features_per_set, embed_dim, num_heads, ff_dim, num_sab_blocks, dropout):\n",
    "    input_surface = tf.keras.Input(shape=(max_sets+1, features_per_set), name='surface_data')\n",
    "    mask = tf.keras.Input(shape=(max_sets,), dtype=tf.float32, name='surface_mask')\n",
    "    \n",
    "    pooled, _ = TransformerEncoder(input_surface, mask, \n",
    "                                embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim, \n",
    "                                dropout=dropout, num_sab_blocks=num_sab_blocks)\n",
    "    \n",
    "    return tf.keras.Model(inputs={\"surface_data\": input_surface, \"surface_mask\": mask}, outputs=pooled, name=\"surface_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83731de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_decoder(embed_dim, num_heads, ff_dim, num_layers, max_sets, features_per_set, dropout):\n",
    "    encoded_input = tf.keras.Input(shape=(1, embed_dim), name='encoded_latent')\n",
    "    \n",
    "    decoded, _ = TransformerDecoder(encoded_input, embed_dim=embed_dim, num_heads=num_heads, \n",
    "                                    ff_dim=ff_dim, num_layers=num_layers, \n",
    "                                    max_sets=max_sets, features_per_set=features_per_set, \n",
    "                                    dropout=dropout)\n",
    "    \n",
    "    return tf.keras.Model(inputs=encoded_input, outputs=decoded, name=\"surface_decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aafdc23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_surface_encoder(hp):\n",
    "    encoder = build_encoder(\n",
    "        max_sets=441, features_per_set=4, embed_dim=hp[\"embed_dim\"],\n",
    "        num_heads=hp[\"num_heads\"], ff_dim=hp[\"ff_dim\"], \n",
    "        num_sab_blocks=hp[\"sab_blocks\"], dropout=hp[\"enc_dropout\"]\n",
    "    )\n",
    "    \n",
    "    decoder = build_decoder(\n",
    "        embed_dim=hp[\"embed_dim\"], num_heads=hp[\"num_heads\"], ff_dim=hp[\"ff_dim\"],\n",
    "        num_layers=hp[\"decoder_blocks\"], max_sets=441, features_per_set=4, dropout=hp[\"dec_dropout\"]\n",
    "    )\n",
    "    \n",
    "    model = SurfaceEncoderModel(encoder, decoder)\n",
    "    return model, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36172cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_hyperparams():\n",
    "    # return {\n",
    "    #     'batch_size': np.random.choice([64, 128, 256]),\n",
    "    #     \"embed_dim\": 64,\n",
    "    #     \"num_heads\": np.random.choice([4, 6, 8]),\n",
    "    #     \"ff_dim\": np.random.choice([128, 256, 512]),\n",
    "    #     \"enc_dropout\": np.random.uniform(0.0, 0.3),\n",
    "    #     \"dec_dropout\": np.random.uniform(0.0, 0.3),\n",
    "    #     \"learning_rate\": 10 ** np.random.uniform(-5, np.log10(3)-3),\n",
    "    #     'weight_decay': 10 ** np.random.uniform(np.log10(5)-3, -2),\n",
    "    #     'sab_blocks': np.random.choice([1,2,3,4,5,6]),\n",
    "    #     'decoder_blocks': np.random.choice([1,2,3,4,5,6])\n",
    "    # }\n",
    "    return {\n",
    "        'batch_size': 64,\n",
    "        \"embed_dim\": 64,\n",
    "        \"num_heads\": 4,\n",
    "        \"ff_dim\": 128,\n",
    "        \"enc_dropout\": np.random.uniform(0.0, 0.3),\n",
    "        \"dec_dropout\": np.random.uniform(0.0, 0.3),\n",
    "        \"learning_rate\": 10 ** np.random.uniform(-5, np.log10(3)-3),\n",
    "        'weight_decay': 10 ** np.random.uniform(np.log10(5)-3, -2),\n",
    "        'sab_blocks': 1,\n",
    "        'decoder_blocks': 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d40f2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model, _, _ = build_surface_encoder(hp)    \n",
    "    optimizer = tf.keras.optimizers.Lion(learning_rate=hp['learning_rate'], weight_decay=hp['weight_decay'])\n",
    "    model.compile(optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac4a614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trial(hp, train_dataset, val_dataset, trial_id=0, epochs=10, use_wandb=False):\n",
    "    callbacks = []\n",
    "    model = build_model(hp)\n",
    "    \n",
    "    # TensorBoard logging\n",
    "    tb_logdir = f\"logs/trial_{trial_id}\"\n",
    "    callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=tb_logdir))\n",
    "\n",
    "    # Optional: wandb logging\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        from wandb.keras import WandbCallback\n",
    "        wandb.init(project=\"coil_autoencoder\", config=hp, name=f\"trial_{trial_id}\")\n",
    "        callbacks.append(WandbCallback())\n",
    "        #attn_map = attn_weights[0].numpy()  # (num_queries, encoded_len)\n",
    "        #wandb.log({\"attention_heatmap\": wandb.Image(attn_map)})\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    val_loss = model.evaluate(val_dataset)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd185301",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=0)  # Set num_gpus=0 if you want CPU-only runs\n",
    "def parallel_train_trial(hp, trial_id, use_wandb):\n",
    "    train, val, test = load_split_datasets(\n",
    "        tfrecord_dir, batch_size=hp['batch_size'], train_frac=0.9, val_frac=0.1\n",
    "    )\n",
    "    val_loss = train_trial(hp, train, val, trial_id, use_wandb=use_wandb)\n",
    "    return val_loss, hp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c80a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_search_parallel(n_trials=10, use_wandb=False):\n",
    "    ray.init(ignore_reinit_error=True)\n",
    "\n",
    "    result_refs = []\n",
    "    for trial_id in range(n_trials):\n",
    "        hp = sample_hyperparams()\n",
    "        ref = parallel_train_trial.remote(hp, trial_id, use_wandb)\n",
    "        result_refs.append(ref)\n",
    "\n",
    "    results = ray.get(result_refs)  # This blocks until all trials are done\n",
    "\n",
    "    # Sort by val_loss\n",
    "    results.sort(key=lambda x: x[0][0])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd245c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_search(n_trials=10, use_wandb=True, save_dir=\"saved_models\"):\n",
    "    results = []\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for trial_id in range(n_trials):\n",
    "        hp = sample_hyperparams()\n",
    "        train, val, test = load_split_datasets(tfrecord_dir, batch_size=hp['batch_size'], train_frac=0.9)\n",
    "        val_loss = train_trial(hp, train, val, trial_id, use_wandb=use_wandb)\n",
    "        save_path = os.path.join(save_dir, f\"trial_{trial_id:03d}_val_{val_loss[0]:.4f}\")\n",
    "        #model.save(save_path)\n",
    "        results.append((val_loss, hp, save_path))\n",
    "\n",
    "    results.sort(key=lambda x: x[0])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd8814a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda3/envs/TFCoil/lib/python3.9/site-packages/keras/src/ops/nn.py:944: UserWarning: You are using a softmax over axis 3 of a tensor of shape (None, 4, 442, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     43/Unknown \u001b[1m74s\u001b[0m 2s/step - coil_latent_loss: 0.1152 - loss: 0.6350 - mae: 0.5179 - recon_loss: 0.5774 - scaler_loss: 0.2071 - unmasked_mse: 0.4640"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/anaconda3/envs/TFCoil/lib/python3.9/site-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 2s/step - coil_latent_loss: 0.1157 - loss: 0.6279 - mae: 0.5155 - recon_loss: 0.5700 - scaler_loss: 0.2052 - unmasked_mse: 0.4594 - val_coil_latent_loss: 0.1636 - val_loss: 0.2584 - val_mae: 0.3488 - val_recon_loss: 0.1766 - val_scaler_loss: 0.0432 - val_unmasked_mse: 0.1910\n",
      "Epoch 2/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 2s/step - coil_latent_loss: 0.1433 - loss: 0.3086 - mae: 0.3771 - recon_loss: 0.2369 - scaler_loss: 0.0716 - unmasked_mse: 0.2274 - val_coil_latent_loss: 0.0878 - val_loss: 0.2144 - val_mae: 0.3464 - val_recon_loss: 0.1705 - val_scaler_loss: 0.0395 - val_unmasked_mse: 0.2030\n",
      "Epoch 3/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - coil_latent_loss: 0.0482 - loss: 0.2166 - mae: 0.3579 - recon_loss: 0.1925 - scaler_loss: 0.0633 - unmasked_mse: 0.2122 - val_coil_latent_loss: 0.0248 - val_loss: 0.1548 - val_mae: 0.2988 - val_recon_loss: 0.1423 - val_scaler_loss: 0.0579 - val_unmasked_mse: 0.1444\n",
      "Epoch 4/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 2s/step - coil_latent_loss: 0.0143 - loss: 0.1474 - mae: 0.2980 - recon_loss: 0.1402 - scaler_loss: 0.0586 - unmasked_mse: 0.1529 - val_coil_latent_loss: 0.0079 - val_loss: 0.0947 - val_mae: 0.2372 - val_recon_loss: 0.0908 - val_scaler_loss: 0.0399 - val_unmasked_mse: 0.1026\n",
      "Epoch 5/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 2s/step - coil_latent_loss: 0.0056 - loss: 0.1018 - mae: 0.2288 - recon_loss: 0.0990 - scaler_loss: 0.0482 - unmasked_mse: 0.0967 - val_coil_latent_loss: 0.0059 - val_loss: 0.0812 - val_mae: 0.1931 - val_recon_loss: 0.0783 - val_scaler_loss: 0.0437 - val_unmasked_mse: 0.0787\n",
      "Epoch 6/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 2s/step - coil_latent_loss: 0.0059 - loss: 0.0873 - mae: 0.1970 - recon_loss: 0.0843 - scaler_loss: 0.0461 - unmasked_mse: 0.0778 - val_coil_latent_loss: 0.0047 - val_loss: 0.0666 - val_mae: 0.1691 - val_recon_loss: 0.0643 - val_scaler_loss: 0.0390 - val_unmasked_mse: 0.0694\n",
      "Epoch 7/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 2s/step - coil_latent_loss: 0.0033 - loss: 0.0767 - mae: 0.1865 - recon_loss: 0.0751 - scaler_loss: 0.0428 - unmasked_mse: 0.0765 - val_coil_latent_loss: 0.0024 - val_loss: 0.0718 - val_mae: 0.1715 - val_recon_loss: 0.0707 - val_scaler_loss: 0.0423 - val_unmasked_mse: 0.0637\n",
      "Epoch 8/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 2s/step - coil_latent_loss: 0.0019 - loss: 0.0738 - mae: 0.1756 - recon_loss: 0.0728 - scaler_loss: 0.0428 - unmasked_mse: 0.0716 - val_coil_latent_loss: 0.0022 - val_loss: 0.0633 - val_mae: 0.1326 - val_recon_loss: 0.0622 - val_scaler_loss: 0.0437 - val_unmasked_mse: 0.0582\n",
      "Epoch 9/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 2s/step - coil_latent_loss: 0.0017 - loss: 0.0692 - mae: 0.1681 - recon_loss: 0.0683 - scaler_loss: 0.0408 - unmasked_mse: 0.0708 - val_coil_latent_loss: 0.0013 - val_loss: 0.0824 - val_mae: 0.1877 - val_recon_loss: 0.0818 - val_scaler_loss: 0.0503 - val_unmasked_mse: 0.0765\n",
      "Epoch 10/10\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 2s/step - coil_latent_loss: 0.0013 - loss: 0.0700 - mae: 0.1606 - recon_loss: 0.0694 - scaler_loss: 0.0437 - unmasked_mse: 0.0663 - val_coil_latent_loss: 8.9890e-04 - val_loss: 0.0567 - val_mae: 0.1286 - val_recon_loss: 0.0563 - val_scaler_loss: 0.0389 - val_unmasked_mse: 0.0635\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618ms/step - coil_latent_loss: 9.0928e-04 - loss: 0.0573 - mae: 0.1289 - recon_loss: 0.0569 - scaler_loss: 0.0394 - unmasked_mse: 0.0636\n"
     ]
    }
   ],
   "source": [
    "res = run_random_search(n_trials=1, use_wandb=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFCoil",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
