{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ec14512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Download and load QUASR device database\n",
    "import gzip\n",
    "import json\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor\n",
    "import time\n",
    "import numpy as np\n",
    "from simsopt.field import Current\n",
    "from simsopt.geo import SurfaceRZFourier\n",
    "from simsopt._core import load\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a499e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMSOPT_DIR = 'quasr_simsopt_files'\n",
    "LOG_CSV = \"canon_curr_quasr_log.csv\"\n",
    "data_dir = Path(SIMSOPT_DIR)\n",
    "output_dir = Path('canon_curr_surface_coil_tfrecords')\n",
    "os.makedirs(SIMSOPT_DIR, exist_ok=True)\n",
    "\n",
    "MAX_COILS = 6\n",
    "FEATURES_PER_COIL = 100\n",
    "NUM_THREAD_WORKERS = 48\n",
    "NUM_PROCESS_WORKERS = 1\n",
    "CHUNK_SIZE = 10000\n",
    "MAX_RETRIES = 5\n",
    "RETRY_DELAY = 2  # seconds\n",
    "MAX_NFP = 5\n",
    "MAX_COEFS = 441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f92a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://quasr.flatironinstitute.org/database.json.gz\"\n",
    "# print('Downloading device database...')\n",
    "# r = requests.get(url)\n",
    "# r.raise_for_status()\n",
    "\n",
    "# with gzip.open(BytesIO(r.content), 'rt', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# df = pd.DataFrame(**data)\n",
    "# print(f\"Loaded {len(df)} devices.\")\n",
    "\n",
    "# df.to_hdf('QUASR_Stellarators.h5', key = 'full_dataset')\n",
    "df = pd.read_hdf('QUASR_Stellarators.h5', key = 'full_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b644101b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314309 devices match your criteria.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Apply filters to select matching devices\n",
    "filtered = df[\n",
    "    (df[\"Nfourier_coil\"] == 16) &\n",
    "    (df['qs_error'] >= -4) &\n",
    "    # (df[\"max_elongation\"] <= 10) &\n",
    "    # (df[\"aspect_ratio\"] >= 4) & (df[\"aspect_ratio\"] <= 10) &\n",
    "    (df[\"nc_per_hp\"] >= 1) & (df[\"nc_per_hp\"] <= 6) &\n",
    "    (df[\"nfp\"] >= 1) & (df[\"nfp\"] <= 5)\n",
    "].copy()\n",
    "\n",
    "print(f\"{len(filtered)} devices match your criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6899ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simsopt_url(device_id):\n",
    "    pid = device_id.zfill(7)\n",
    "    return f\"https://quasr.flatironinstitute.org/simsopt_serials/{pid[:4]}/serial{pid}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "067ac04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Robust download with retries\n",
    "def download_with_retries(url: str, path: str) -> bool:\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=30)\n",
    "            if r.status_code == 200:\n",
    "                with open(path, 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"{url} returned status {r.status_code} (attempt {attempt})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error on {url} (attempt {attempt}): {e}\")\n",
    "        time.sleep(RETRY_DELAY)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c880e329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314309 devices to download in 32 chunks.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Prepare log and list of device IDs to download\n",
    "if os.path.exists(LOG_CSV):\n",
    "    log_df = pd.read_csv(LOG_CSV, dtype=str)\n",
    "else:\n",
    "    log_df = pd.DataFrame(columns=[\"ID\", 'simsopt_url', \"status\"])\n",
    "\n",
    "processed = set(log_df[\"ID\"])\n",
    "device_ids = [str(d) for d in filtered[\"ID\"] if str(d) not in processed] #this is where you change which df you want the device ids from\n",
    "chunks = [device_ids[i:i+CHUNK_SIZE] for i in range(0, len(device_ids), CHUNK_SIZE)]\n",
    "print(f\"{len(device_ids)} devices to download in {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7969a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_device(dev_id):\n",
    "        pid = dev_id.zfill(7)\n",
    "        # vmec_path = os.path.join(VMEC_DIR, f\"input.{pid}\")\n",
    "        simsopt_path = os.path.join(SIMSOPT_DIR, f\"input_{pid}.json\")\n",
    "        # vmec_ok = os.path.exists(vmec_path) or download_with_retries(vmec_url(dev_id), vmec_path)\n",
    "        simsopt_ok = os.path.exists(simsopt_path) or download_with_retries(simsopt_url(dev_id), simsopt_path)\n",
    "        status = \"success\" if simsopt_ok else 'failed'\n",
    "        return {\n",
    "            \"ID\": dev_id,\n",
    "            # \"vmec_url\": vmec_url(dev_id),\n",
    "            'simsopt_url': simsopt_url(dev_id),\n",
    "            \"status\": status\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea6824a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value.flatten()))\n",
    "\n",
    "def _int_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def serialize_data(id: str, coils: np.ndarray, coil_mask: np.ndarray, surface: np.ndarray, surface_mask: np.ndarray):\n",
    "    feature = {\n",
    "        'ID': _bytes_feature(id.encode('utf-8')),\n",
    "        'coil_data': _float_feature(coils),\n",
    "        'coil_mask': _int_feature(coil_mask),\n",
    "        'surface_data': _float_feature(surface),\n",
    "        'surface_mask': _int_feature(surface_mask)\n",
    "    }\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fc0db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_curve_points(curve, n_samples=200):\n",
    "    \"\"\"\n",
    "    Return (N, 3) array of points on the curve, robust to simsoptpp vs Python API.\n",
    "    - simsoptpp.CurveXYZFourier.gamma(): no-arg, uses internal grid\n",
    "    - Python CurveXYZFourier.gamma(s): accepts an s-array\n",
    "    \"\"\"\n",
    "    # Try simsoptpp form (no-arg)\n",
    "    try:\n",
    "        # (Optional) try to set resolution if the curve exposes a grid attribute.\n",
    "        # Some builds have `curve.quadpoints` you can assign; if not, we just use whatever it has.\n",
    "        pts = curve.gamma()               # should be (N,3) or (3,N)\n",
    "    except TypeError:\n",
    "        # Fall back to Python form that accepts s\n",
    "        s_vals = np.linspace(0.0, 1.0, n_samples, endpoint=False)\n",
    "        pts = curve.gamma(s_vals)\n",
    "\n",
    "    pts = np.asarray(pts)\n",
    "    if pts.ndim != 2:\n",
    "        raise ValueError(f\"curve.gamma returned unexpected shape {pts.shape}\")\n",
    "    # Normalize to (N,3)\n",
    "    if pts.shape[0] == 3 and pts.shape[1] != 3:\n",
    "        pts = pts.T\n",
    "    if pts.shape[1] != 3:\n",
    "        raise ValueError(f\"Expected last dim 3 for xyz; got {pts.shape}\")\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6989cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coil_centroid_and_phi(coil, n_samples=200):\n",
    "    \"\"\"Sample the coil curve and compute centroid and toroidal angle φ of the centroid.\"\"\"\n",
    "    curve = getattr(coil, \"curve\", coil)  # sometimes `coil` is already a curve\n",
    "    pts = sample_curve_points(curve, n_samples=n_samples)\n",
    "    xbar, ybar, zbar = pts[:, 0].mean(), pts[:, 1].mean(), pts[:, 2].mean()\n",
    "    phi = (np.arctan2(ybar, xbar) + 2*np.pi) % (2*np.pi)\n",
    "    return xbar, ybar, zbar, phi\n",
    "\n",
    "def _coil_current_scalar(coil):\n",
    "    # Use same access you used earlier for consistency. Adjust if needed.\n",
    "    # Fallbacks (commented) are typical simsopt APIs if your object differs.\n",
    "    try:\n",
    "        return float(coil.current.current_to_scale.current)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return float(coil.current.get_value()/(795774.7154594767))\n",
    "        except Exception:\n",
    "            return float(coil.current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_coils(file_path):\n",
    "    try:\n",
    "        id = str(file_path)[-12:-5]\n",
    "        surfaces, coils = load(str(file_path))\n",
    "        s = surfaces[-1]\n",
    "\n",
    "        # --- compute per-coil stats ---\n",
    "        nfp = int(s.nfp)\n",
    "        sector_width = 2*np.pi / nfp\n",
    "\n",
    "        stats = []\n",
    "        for idx, coil in enumerate(coils):\n",
    "            xbar, ybar, zbar, phi = _coil_centroid_and_phi(coil, n_samples=200)\n",
    "            k = int(np.floor(phi / sector_width)) % nfp             # sector index\n",
    "            phi_red = phi - k * sector_width                        # reduced angle in [0, sector_width)\n",
    "            I = _coil_current_scalar(coil)\n",
    "            stats.append({\n",
    "                \"idx\": idx,\n",
    "                \"phi\": phi,\n",
    "                \"phi_red\": phi_red,\n",
    "                \"sector\": k,\n",
    "                \"zbar\": zbar,\n",
    "                \"I\": I,\n",
    "            })\n",
    "\n",
    "        # --- pick a single sector to remove toroidal copies ---\n",
    "        chosen_sector = 0\n",
    "        sel = [st for st in stats if st[\"sector\"] == chosen_sector]\n",
    "        if not sel:\n",
    "            # fallback: choose sector with most coils\n",
    "            counts = np.bincount([st[\"sector\"] for st in stats], minlength=nfp)\n",
    "            chosen_sector = int(np.argmax(counts))\n",
    "            sel = [st for st in stats if st[\"sector\"] == chosen_sector]\n",
    "\n",
    "        # --- canonical sort inside chosen sector ---\n",
    "        sel.sort(key=lambda st: (st[\"phi_red\"], -st[\"zbar\"], -np.sign(st[\"I\"]), -abs(st[\"I\"])))\n",
    "\n",
    "        # --- how many coils to keep? ---\n",
    "        # If your intention was \"unique shapes per (nfp*mirror)\", selecting one sector already removed nfp.\n",
    "        # We now KEEP both mirror partners (upper/lower) unless you truly only want one of them.\n",
    "        num_coils = min(len(sel), MAX_COILS)\n",
    "\n",
    "        coil_array = np.zeros((MAX_COILS, FEATURES_PER_COIL), dtype=np.float32)\n",
    "        order_indices = []  # permutation: canonical position -> original coils[] index\n",
    "\n",
    "        for dst_i in range(num_coils):\n",
    "            src_idx = sel[dst_i][\"idx\"]\n",
    "            # your original packing: first element is current, then last 99 Fourier params\n",
    "            params = coils[src_idx].x[-99:]  # adjust if your feature layout changes\n",
    "            curr = np.array(_coil_current_scalar(coils[src_idx]), dtype=np.float32)\n",
    "            coil_array[dst_i] = np.concatenate([curr[None], params.astype(np.float32)], axis=0)\n",
    "            order_indices.append(src_idx)\n",
    "\n",
    "        coil_mask = np.array([1]*num_coils + [0]*(MAX_COILS - num_coils), dtype=np.int64)\n",
    "\n",
    "        # Optional: return order_indices if you want to log them or store in TFRecord\n",
    "        return id, coil_array, coil_mask  # , order_indices\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f853cfc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def process_coils(file_path):\\n    try:\\n        id = str(file_path)[-12:-5]\\n        surfaces, coils = load(str(file_path))\\n        s = surfaces[-1]\\n\\n        num_coils = len(coils) // (s.nfp * 2)\\n        num_coils = min(num_coils, MAX_COILS)\\n\\n        # log_scaler = np.log10(coils[0].current.scale)\\n        # scaler_token = np.full((1, FEATURES_PER_COIL), log_scaler, dtype=np.float32)\\n\\n        coil_array = np.zeros((MAX_COILS, FEATURES_PER_COIL), dtype=np.float32)\\n        for i in range(num_coils):\\n            params = coils[i].x[-99:]  # 99 Fourier + 1 current\\n            curr = np.array(coils[i].current.current_to_scale.current)\\n            coil_array[i] = np.append(curr, params)\\n\\n        # coil_array[-1] = scaler_token  # context token\\n\\n        coil_mask = np.array([1] * num_coils + [0] * (MAX_COILS - num_coils), dtype=np.int64)\\n\\n        return id, coil_array, coil_mask #serialize_coil(id, coil_array, coil_mask)\\n        \\n    except Exception as e:\\n        print(f\"Failed on {file_path}: {e}\")\\n        return None    '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def process_coils(file_path):\n",
    "    try:\n",
    "        id = str(file_path)[-12:-5]\n",
    "        surfaces, coils = load(str(file_path))\n",
    "        s = surfaces[-1]\n",
    "\n",
    "        num_coils = len(coils) // (s.nfp * 2)\n",
    "        num_coils = min(num_coils, MAX_COILS)\n",
    "\n",
    "        # log_scaler = np.log10(coils[0].current.scale)\n",
    "        # scaler_token = np.full((1, FEATURES_PER_COIL), log_scaler, dtype=np.float32)\n",
    "\n",
    "        coil_array = np.zeros((MAX_COILS, FEATURES_PER_COIL), dtype=np.float32)\n",
    "        for i in range(num_coils):\n",
    "            params = coils[i].x[-99:]  # 99 Fourier + 1 current\n",
    "            curr = np.array(coils[i].current.current_to_scale.current)\n",
    "            coil_array[i] = np.append(curr, params)\n",
    "\n",
    "        # coil_array[-1] = scaler_token  # context token\n",
    "\n",
    "        coil_mask = np.array([1] * num_coils + [0] * (MAX_COILS - num_coils), dtype=np.int64)\n",
    "\n",
    "        return id, coil_array, coil_mask #serialize_coil(id, coil_array, coil_mask)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {file_path}: {e}\")\n",
    "        return None    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0320b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_surface(file_path):\n",
    "    \"\"\"\n",
    "    Convert s.x and metadata to [N_modes, 5] array:\n",
    "    columns = [m_norm, n_norm, is_cos, is_R, coeff_value]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        id = str(file_path)[-12:-5]\n",
    "        surfaces, coils = load(str(file_path))\n",
    "        outer_surface = surfaces[-1]\n",
    "        s = outer_surface.to_RZFourier() #the bottleneck in speed\n",
    "\n",
    "        x = s.x  # the coeff vector\n",
    "        nfp = s.nfp\n",
    "\n",
    "        num_coefs = len(x)\n",
    "        if num_coefs > MAX_COEFS:\n",
    "            print(num_coefs)\n",
    "        \n",
    "        m = s.m  # mode numbers, shape (N_modes,)\n",
    "        n = s.n\n",
    "        \n",
    "        num_modes = (len(m)+1)//2\n",
    "        # Normalize mode indices\n",
    "        max_m = np.max(np.abs(m)) or 1\n",
    "        max_n = np.max(np.abs(n)) or 1\n",
    "\n",
    "        # Type flags\n",
    "        is_cos = np.concatenate([\n",
    "            np.ones(num_modes, dtype=bool),  # R_cos\n",
    "            np.zeros(num_modes-1, dtype=bool)  # Z_sin\n",
    "        ])\n",
    "\n",
    "        surface_set = np.zeros((MAX_COEFS+1, 4), dtype=np.float32)\n",
    "        surface_mask = np.zeros((MAX_COEFS,), dtype=np.int64)\n",
    "        nfp_norm = float(nfp) / MAX_NFP \n",
    "\n",
    "        for i in range(min(num_coefs, MAX_COEFS)):\n",
    "            surface_set[i] = [\n",
    "                m[i] / max_m,\n",
    "                n[i] / max_n,\n",
    "                float(is_cos[i]),\n",
    "                x[i]\n",
    "            ]\n",
    "            surface_mask[i] = 1\n",
    "\n",
    "        surface_set[-1] = nfp_norm\n",
    "            \n",
    "        return surface_set, surface_mask #serialize_surface(id, surface_set, surface_mask)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1442a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    id, coil_array, coil_mask = process_coils(file_path)\n",
    "\n",
    "    try: \n",
    "        surface_set, surface_mask = process_surface(file_path)\n",
    "        return serialize_data(id, coil_array, coil_mask, surface_set, surface_mask)\n",
    "    except Exception as e:\n",
    "        print(f'{file_path} failure handled.')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf6137cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tfrecord_chunk(serialized_examples, output_path):\n",
    "    with tf.io.TFRecordWriter(str(output_path)) as writer:\n",
    "        for ex in serialized_examples:\n",
    "            if ex:\n",
    "                writer.write(ex)\n",
    "\n",
    "def datasets_to_tfrecords(directory: Path, output_dir: Path, idx,\n",
    "                               chunk_size=CHUNK_SIZE, num_workers=NUM_PROCESS_WORKERS):\n",
    "    files = list(directory.glob(\"*.json\"))\n",
    "    total_files = len(files)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
    "        serialized_examples = list(tqdm(\n",
    "            executor.map(process_file, files),\n",
    "            total=total_files,\n",
    "            desc=f\"Chunk {idx//chunk_size:03d}\"\n",
    "        ))\n",
    "\n",
    "    serialized_examples = [ex for ex in serialized_examples if ex is not None]\n",
    "\n",
    "    output_path = output_dir / f\"surface_coil_chunk_{idx:03d}.tfrecord\"\n",
    "\n",
    "    write_tfrecord_chunk(serialized_examples, output_path)\n",
    "    print(f\"✅ Saved {len(serialized_examples)} surface coil pair samples to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f18eadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Chunk 0/32: 10000 devices ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 0: 100%|██████████| 10000/10000 [00:00<00:00, 159159.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledCurrent1\n",
      "-474110.27804410254\n",
      "795774.7154594767\n",
      "ScaledCurrent2-173421.57760046108\n",
      "\n",
      "795774.7154594767\n",
      "ScaledCurrent3\n",
      "-1446704.549733777\n",
      "795774.7154594767\n",
      "ScaledCurrent4\n",
      "615659.3276062163\n",
      "795774.7154594767\n",
      "ScaledCurrent12\n",
      "-615659.3276062163\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0029216.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent13\n",
      "-529964.6638656477\n",
      "795774.7154594767\n",
      "ScaledCurrent14\n",
      "-1061612.5030601579\n",
      "795774.7154594767\n",
      "ScaledCurrent18\n",
      "1061612.5030601579\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0014533.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent19\n",
      "-785890.4302878877\n",
      "795774.7154594767\n",
      "ScaledCurrent20\n",
      "-777162.4549206169\n",
      "795774.7154594767\n",
      "ScaledCurrent24\n",
      "777162.4549206169\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0011936.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent25\n",
      "-442116.3756034946\n",
      "795774.7154594767\n",
      "ScaledCurrent26\n",
      "-508184.24917048385\n",
      "795774.7154594767\n",
      "ScaledCurrent30\n",
      "508184.24917048385\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0013799.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent31\n",
      "-639796.703092261\n",
      "795774.7154594767\n",
      "ScaledCurrent32\n",
      "-325958.6538313267\n",
      "795774.7154594767\n",
      "ScaledCurrent33\n",
      "-311021.56272226985\n",
      "795774.7154594767\n",
      "ScaledCurrent39\n",
      "311021.56272226985\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0024679.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent40\n",
      "-205027.292824664\n",
      "795774.7154594767\n",
      "ScaledCurrent41\n",
      "-141049.26362607905\n",
      "795774.7154594767\n",
      "ScaledCurrent42\n",
      "-21711.35962054299\n",
      "795774.7154594767\n",
      "ScaledCurrent43\n",
      "-55513.55342813847\n",
      "795774.7154594767\n",
      "ScaledCurrent51\n",
      "55513.55342813847\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0030069.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent52\n",
      "-321233.88538006047\n",
      "795774.7154594767\n",
      "ScaledCurrent53\n",
      "-40458.7597393654\n",
      "795774.7154594767\n",
      "ScaledCurrent54\n",
      "-123278.01687292902\n",
      "795774.7154594767\n",
      "ScaledCurrent55\n",
      "-310514.1120606987\n",
      "795774.7154594767\n",
      "ScaledCurrent63\n",
      "310514.1120606987\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0029526.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent64\n",
      "-498500.58041008096\n",
      "795774.7154594767\n",
      "ScaledCurrent65\n",
      "-592052.1970761922\n",
      "795774.7154594767\n",
      "ScaledCurrent69\n",
      "592052.1970761922\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0012147.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent70\n",
      "-9853.31347802534\n",
      "795774.7154594767\n",
      "ScaledCurrent71\n",
      "-30858.461811074456\n",
      "795774.7154594767\n",
      "ScaledCurrent72\n",
      "-5271.052064821153\n",
      "795774.7154594767\n",
      "ScaledCurrent73\n",
      "-19947.225670339234\n",
      "795774.7154594767\n",
      "ScaledCurrent81\n",
      "19947.225670339234\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0030141.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent82\n",
      "-795908.1833283082\n",
      "795774.7154594767\n",
      "ScaledCurrent83\n",
      "-1106845.6524056974\n",
      "795774.7154594767\n",
      "ScaledCurrent87\n",
      "1106845.6524056974\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0011416.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent88\n",
      "-1352802.978209576\n",
      "795774.7154594767\n",
      "ScaledCurrent90\n",
      "1352802.978209576\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0005217.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent91\n",
      "-714935.9187428359\n",
      "795774.7154594767\n",
      "ScaledCurrent92\n",
      "-975743.1984510152\n",
      "795774.7154594767\n",
      "ScaledCurrent96\n",
      "975743.1984510152\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0012658.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent97\n",
      "-1341781.8023963412\n",
      "795774.7154594767\n",
      "ScaledCurrent99\n",
      "1341781.8023963412\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0004116.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent100\n",
      "\n",
      "-694520.2320317987795774.7154594767\n",
      "ScaledCurrent101\n",
      "-886957.8953917583\n",
      "795774.7154594767\n",
      "ScaledCurrent105\n",
      "886957.8953917583\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0017459.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent106\n",
      "-1328405.1050363549\n",
      "795774.7154594767\n",
      "ScaledCurrent108\n",
      "1328405.1050363549\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0003124.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent109\n",
      "-724954.3216768266\n",
      "795774.7154594767\n",
      "ScaledCurrent110\n",
      "-527759.0084363327\n",
      "795774.7154594767\n",
      "ScaledCurrent114\n",
      "527759.0084363327\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0011028.json: 'ScaledCurrent' object has no attribute 'current'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 000:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledCurrent115\n",
      "-636707.7612317706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunk 000:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "795774.7154594767\n",
      "ScaledCurrent116\n",
      "-363659.71078046307\n",
      "795774.7154594767\n",
      "ScaledCurrent120\n",
      "363659.71078046307\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0010587.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent121\n",
      "-34784.83249089118\n",
      "795774.7154594767\n",
      "ScaledCurrent122\n",
      "-229484.9695716044\n",
      "795774.7154594767\n",
      "ScaledCurrent123\n",
      "-174559.10118729185\n",
      "795774.7154594767\n",
      "ScaledCurrent124\n",
      "-200458.52555982283\n",
      "795774.7154594767\n",
      "ScaledCurrent125\n",
      "-122541.93477075105\n",
      "795774.7154594767\n",
      "ScaledCurrent126\n",
      "-198860.80622157574\n",
      "795774.7154594767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledCurrent157\n",
      "-211079.76185047996\n",
      "795774.7154594767\n",
      "ScaledCurrent158\n",
      "-168944.5011542582\n",
      "795774.7154594767\n",
      "ScaledCurrent159\n",
      "-74176.83828056444\n",
      "795774.7154594767\n",
      "ScaledCurrent165\n",
      "74176.83828056444\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0024261.json: 'ScaledCurrent' object has no attribute 'current'\n",
      "ScaledCurrent166\n",
      "-837423.7328793893\n",
      "795774.7154594767\n",
      "ScaledCurrent169\n",
      "837423.7328793893\n",
      "-1.0\n",
      "Failed on quasr_simsopt_files/input_0042377.json: 'ScaledCurrent' object has no attribute 'current'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/exouser/anaconda3/envs/coilopt/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/home/exouser/anaconda3/envs/coilopt/lib/python3.10/concurrent/futures/process.py\", line 205, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/home/exouser/anaconda3/envs/coilopt/lib/python3.10/concurrent/futures/process.py\", line 205, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"/tmp/ipykernel_18238/3529407135.py\", line 2, in process_file\n    id, coil_array, coil_mask = process_coils(file_path)\nTypeError: cannot unpack non-iterable NoneType object\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fut \u001b[38;5;129;01min\u001b[39;00m tqdm(as_completed(futures), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(futures), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(fut\u001b[38;5;241m.\u001b[39mresult())\n\u001b[0;32m---> 12\u001b[0m \u001b[43mdatasets_to_tfrecords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m log_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([log_df, pd\u001b[38;5;241m.\u001b[39mDataFrame(results)], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m log_df\u001b[38;5;241m.\u001b[39mto_csv(LOG_CSV, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m, in \u001b[0;36mdatasets_to_tfrecords\u001b[0;34m(directory, output_dir, idx, chunk_size, num_workers)\u001b[0m\n\u001b[1;32m     11\u001b[0m output_dir\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mnum_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m---> 14\u001b[0m     serialized_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChunk \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;132;43;01m:\u001b[39;49;00m\u001b[38;5;124;43m03d\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m serialized_examples \u001b[38;5;241m=\u001b[39m [ex \u001b[38;5;28;01mfor\u001b[39;00m ex \u001b[38;5;129;01min\u001b[39;00m serialized_examples \u001b[38;5;28;01mif\u001b[39;00m ex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     22\u001b[0m output_path \u001b[38;5;241m=\u001b[39m output_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurface_coil_chunk_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tfrecord\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/coilopt/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/coilopt/lib/python3.10/concurrent/futures/process.py:575\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    576\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m~/anaconda3/envs/coilopt/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/anaconda3/envs/coilopt/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/anaconda3/envs/coilopt/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/anaconda3/envs/coilopt/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "for idx, chunk in enumerate(chunks, start=0):\n",
    "    print(f\"\\n=== Chunk {idx}/{len(chunks)}: {len(chunk)} devices ===\")\n",
    "    results = []\n",
    "\n",
    "    os.makedirs(SIMSOPT_DIR, exist_ok=True)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=NUM_THREAD_WORKERS) as executor:\n",
    "        futures = {executor.submit(process_device, dev): dev for dev in chunk}\n",
    "        for fut in tqdm(as_completed(futures), total=len(futures), desc=f\"Chunk {idx}\"):\n",
    "            results.append(fut.result())\n",
    "    \n",
    "    datasets_to_tfrecords(directory=data_dir, output_dir=output_dir, idx=idx)\n",
    "\n",
    "    log_df = pd.concat([log_df, pd.DataFrame(results)], ignore_index=True)\n",
    "    log_df.to_csv(LOG_CSV, index=False)\n",
    "    success = sum(r[\"status\"] == \"success\" for r in results)\n",
    "    print(f\"Chunk {idx} completed: {success}/{len(results)} successful.\")\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(SIMSOPT_DIR)\n",
    "    except OSError as e:\n",
    "        print(f'Error deleting directory: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coilopt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
